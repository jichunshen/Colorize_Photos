{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'keras' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "#Note: You need to reset the kernel for the keras installation to take place\n",
    "#Todo: Remove this line once it is installed, reset the kernel: Menu > Kernel > Reset & Clear Output\n",
    "#!git clone https://github.com/fchollet/keras.git && cd keras && python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SpringCurry/anaconda3/envs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "# Change to '/data/images/Train/' to use all the 10k images\n",
    "X = []\n",
    "for filename in os.listdir('colornet/'):\n",
    "    X.append(img_to_array(load_img('colornet/'+filename)))\n",
    "X = np.array(X, dtype=float)\n",
    "Xtrain = 1.0/255*X\n",
    "\n",
    "#Load weights\n",
    "inception = InceptionResNetV2(weights=None, include_top=True)\n",
    "inception.load_weights('inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_input = Input(shape=(1000,))\n",
    "\n",
    "print(embed_input)\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SpringCurry/anaconda3/envs/ml/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 159s 8s/step - loss: 0.0637\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 144s 7s/step - loss: 0.0146\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 143s 7s/step - loss: 0.0146\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 159s 8s/step - loss: 0.0144\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 142s 7s/step - loss: 0.0139\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 143s 7s/step - loss: 0.0136\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 160s 8s/step - loss: 0.0132\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 149s 7s/step - loss: 0.0123\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 174s 9s/step - loss: 0.0115\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 155s 8s/step - loss: 0.0101\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 148s 7s/step - loss: 0.0097\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 158s 8s/step - loss: 0.0086\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 147s 7s/step - loss: 0.0085\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.0075\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 146s 7s/step - loss: 0.0067\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 146s 7s/step - loss: 0.0059\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 144s 7s/step - loss: 0.0055\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 146s 7s/step - loss: 0.0049\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 146s 7s/step - loss: 0.0059\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 162s 8s/step - loss: 0.0049\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 159s 8s/step - loss: 0.0042\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 822s 41s/step - loss: 0.0038\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 156s 8s/step - loss: 0.0037\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 151s 8s/step - loss: 0.0035\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 160s 8s/step - loss: 0.0034\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 154s 8s/step - loss: 0.0032\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 142s 7s/step - loss: 0.0030\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 115s 6s/step - loss: 0.0028\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 241s 12s/step - loss: 0.0028\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 327s 16s/step - loss: 0.0026\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 300s 15s/step - loss: 0.0027\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 212s 11s/step - loss: 0.0025\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 227s 11s/step - loss: 0.0023\n",
      "Epoch 34/100\n",
      " 1/20 [>.............................] - ETA: 4:32 - loss: 0.0024"
     ]
    }
   ],
   "source": [
    "#Create embedding\n",
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "\n",
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "#Generate training data\n",
    "batch_size = 20\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        embed = create_inception_embedding(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n",
    "\n",
    "#Train model      \n",
    "tensorboard = TensorBoard(log_dir=\"output\")\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=100, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"color_tensorflow_real_mode.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on validation images\n",
    "# Change to '/data/images/Test/' to use all the 500 test images\n",
    "color_me = []\n",
    "for filename in os.listdir('../Test/'):\n",
    "    color_me.append(img_to_array(load_img('../Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "color_me = 1.0/255*color_me\n",
    "color_me = gray2rgb(rgb2gray(color_me))\n",
    "color_me_embed = create_inception_embedding(color_me)\n",
    "color_me = rgb2lab(color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
